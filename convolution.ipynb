{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Neural Network\n",
    "This notebook is quick CNN (convolution neual network) predicting digit for [kaggle competition](https://www.kaggle.com/c/digit-recognizer). \n",
    "\n",
    "Please consider follow before reading this notebook.\n",
    "- We don't verify data in this notebook.\n",
    "- I would like to make this notebook simple enough for anyone who new to CNN can follow the code easily.\n",
    "- I also provide MLP (multilayer perceptron) code with the same problem. Please take a look if you new to Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "train_dataset = pd.read_csv('dataset/train.csv')\n",
    "test_dataset = pd.read_csv('dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset\n",
      "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
      "0      1       0       0       0       0       0       0       0       0   \n",
      "1      0       0       0       0       0       0       0       0       0   \n",
      "2      1       0       0       0       0       0       0       0       0   \n",
      "3      4       0       0       0       0       0       0       0       0   \n",
      "4      0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
      "0       0    ...            0         0         0         0         0   \n",
      "1       0    ...            0         0         0         0         0   \n",
      "2       0    ...            0         0         0         0         0   \n",
      "3       0    ...            0         0         0         0         0   \n",
      "4       0    ...            0         0         0         0         0   \n",
      "\n",
      "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
      "0         0         0         0         0         0  \n",
      "1         0         0         0         0         0  \n",
      "2         0         0         0         0         0  \n",
      "3         0         0         0         0         0  \n",
      "4         0         0         0         0         0  \n",
      "\n",
      "[5 rows x 785 columns]\n",
      "Test Dataset\n",
      "   pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
      "0       0       0       0       0       0       0       0       0       0   \n",
      "1       0       0       0       0       0       0       0       0       0   \n",
      "2       0       0       0       0       0       0       0       0       0   \n",
      "3       0       0       0       0       0       0       0       0       0   \n",
      "4       0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel9    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
      "0       0    ...            0         0         0         0         0   \n",
      "1       0    ...            0         0         0         0         0   \n",
      "2       0    ...            0         0         0         0         0   \n",
      "3       0    ...            0         0         0         0         0   \n",
      "4       0    ...            0         0         0         0         0   \n",
      "\n",
      "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
      "0         0         0         0         0         0  \n",
      "1         0         0         0         0         0  \n",
      "2         0         0         0         0         0  \n",
      "3         0         0         0         0         0  \n",
      "4         0         0         0         0         0  \n",
      "\n",
      "[5 rows x 784 columns]\n",
      "Train Size : 42000\n",
      "Test Size : 28000\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Dataset\")\n",
    "print(train_dataset[:5])\n",
    "print(\"Test Dataset\")\n",
    "print(test_dataset[:5])\n",
    "print(\"Train Size : {}\".format(train_dataset.shape[0]))\n",
    "print(\"Test Size : {}\".format(test_dataset.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data\n",
    "Before training we need to prepare data a little bit. here is the detail\n",
    "- We need to scale each pixel to the range of 0 - 1\n",
    "- Since Kaggle don't have validate data for us,so we have to split some from training data.\n",
    "- We need to spearate label (train_y) out of feature (train_x) so we can feed it to our network as label and feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 784)\n",
      "Train Size : 37800\n",
      "Validation Size : 4200\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "validation_ratio = 0.1\n",
    "\n",
    "pixel_scaler = MinMaxScaler()\n",
    "one_hot = LabelBinarizer()\n",
    "one_hot.fit(range(10))\n",
    "train_y = one_hot.transform(train_dataset['label'])\n",
    "train_x = train_dataset.drop('label', axis=1)\n",
    "print(train_x.values.shape)\n",
    "train_x = pixel_scaler.fit_transform(train_x)\n",
    "train_x = train_x.reshape( (-1, 28, 28, 1) )\n",
    "test_x = pixel_scaler.transform(test_dataset)\n",
    "test_x = test_x.reshape( (-1, 28, 28, 1) )\n",
    "\n",
    "split = int(train_dataset.shape[0] * (1 - validation_ratio))\n",
    "\n",
    "valid_x = train_x[split:train_x.shape[0]]\n",
    "valid_y = train_y[split:train_x.shape[0]]\n",
    "train_x = train_x[0:split]\n",
    "train_y = train_y[0:split]\n",
    "print(\"Train Size : {}\".format(train_x.shape[0]))\n",
    "print(\"Validation Size : {}\".format(valid_x.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Model\n",
    "Below is the model code.\n",
    "- Initialize DigitalRecognizerCNN at first.\n",
    "- Call make_nn() to set up tensorflow graph.\n",
    "- Call train() for train network after training the network has been stored and ready for prediction.\n",
    "- Call predict() for make a prediction with the latest saved network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitRecognizerCNN(object):\n",
    "    def __init__(self, width=28, height=28, channel=1, label_size=10):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.channel = channel\n",
    "        self.label_size = label_size\n",
    "    \n",
    "    def build_input(self):\n",
    "        model_x = tf.placeholder(tf.float32, [None, self.width, self.height, self.channel])\n",
    "        model_y = tf.placeholder(tf.int32, [None, self.label_size])\n",
    "        model_lr = tf.placeholder(tf.float32)\n",
    "        return model_x, model_y, model_lr\n",
    "    \n",
    "    def build_cnn(self, model_x, model_y):\n",
    "        model_x = tf.reshape(model_x, (-1, 28, 28, 1))\n",
    "        \n",
    "        conv_1 = tf.layers.conv2d(model_x, 3, 16, padding='same', activation=tf.nn.relu)\n",
    "        pooling_1 = tf.layers.max_pooling2d(conv_1, 2, 1, padding='same')\n",
    "        conv_2 = tf.layers.conv2d(pooling_1, 3, 32, padding='same', activation=tf.nn.relu)\n",
    "        pooling_2 = tf.layers.max_pooling2d(conv_2, 2, 1, padding='same')\n",
    "        flatten = tf.layers.flatten(pooling_2)\n",
    "        logits = tf.layers.dense(flatten, self.label_size, activation=None)\n",
    "        return logits\n",
    "    \n",
    "    def build_output(self, model_y, logits):\n",
    "        output = tf.argmax(logits, axis=1)\n",
    "        loss = tf.losses.softmax_cross_entropy(model_y, logits)\n",
    "        accuracy = tf.reduce_mean( tf.cast( tf.equal( tf.argmax(model_y, axis=1), output ), tf.float32) )\n",
    "        return output, loss, accuracy\n",
    "    \n",
    "    def model_opt(self, model_lr, loss):\n",
    "        opt = tf.train.AdamOptimizer(model_lr).minimize(loss)\n",
    "        return opt\n",
    "    \n",
    "    def make_nn(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.model_x, self.model_y, self.model_lr = self.build_input()\n",
    "        self.logits = self.build_cnn(self.model_x, self.model_y)\n",
    "        self.output, self.loss, self.accuracy = self.build_output(self.model_y, self.logits)\n",
    "        self.opt = self.model_opt(self.model_lr, self.loss)\n",
    "        \n",
    "    def train(self, epoch, learning_rate, train_x, train_y, batch_size, valid_x, valid_y, get_batch_func, print_every):\n",
    "        t_loss_list = []\n",
    "        v_loss_list = []\n",
    "        accuracy_list = []\n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session() as sess :\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            counter = 0\n",
    "            for e in range(epoch) :\n",
    "                for x,y in get_batch_func(train_x, train_y, batch_size):\n",
    "                    feed_dict = {self.model_x:x, self.model_y:y, self.model_lr:learning_rate}\n",
    "                    loss, _ = sess.run([self.loss, self.opt], feed_dict=feed_dict)\n",
    "                    if counter % print_every == 0 :\n",
    "                        feed_dict = {self.model_x:valid_x, self.model_y:valid_y, self.model_lr:learning_rate}\n",
    "                        v_loss, accuracy = sess.run([self.loss, self.accuracy], feed_dict=feed_dict)\n",
    "                        print(\"Epoch: {}/{}, Step: {}, T Loss: {:.4f}, V Loss: {:.4f}, Accuracy: {:.4f}\".format(e+1, \n",
    "                                    epoch, counter, loss, v_loss, accuracy))\n",
    "                        t_loss_list.append( loss )\n",
    "                        v_loss_list.append( v_loss )\n",
    "                        accuracy_list.append( accuracy )\n",
    "                        save_path = saver.save(sess, \"tensor/model.ckpt\")\n",
    "                    counter += 1\n",
    "        return t_loss_list, v_loss_list, accuracy_list\n",
    "    \n",
    "    def predict(self, predict_x):\n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver.restore(sess, \"tensor/model.ckpt\")\n",
    "            feed_dict = {self.model_x:predict_x}\n",
    "            output = sess.run(self.output, feed_dict=feed_dict)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Testing --- Delete after use\n",
    "dr_cnn = DigitRecognizerCNN()\n",
    "dr_cnn.make_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(feature, label, batch_size):\n",
    "    num_batch = feature.shape[0] // batch_size\n",
    "    for i in range(num_batch):\n",
    "        start = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "        x = feature[start:end]\n",
    "        y = label[start:end]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20, Step: 0, T Loss: 2.2921, V Loss: 2.2969, Accuracy: 0.1050\n",
      "Epoch: 1/20, Step: 1000, T Loss: 0.2663, V Loss: 0.2948, Accuracy: 0.9169\n",
      "Epoch: 2/20, Step: 2000, T Loss: 0.2387, V Loss: 0.2060, Accuracy: 0.9393\n",
      "Epoch: 3/20, Step: 3000, T Loss: 0.1466, V Loss: 0.1598, Accuracy: 0.9507\n",
      "Epoch: 4/20, Step: 4000, T Loss: 0.0679, V Loss: 0.1356, Accuracy: 0.9583\n",
      "Epoch: 5/20, Step: 5000, T Loss: 0.1587, V Loss: 0.1219, Accuracy: 0.9617\n",
      "Epoch: 6/20, Step: 6000, T Loss: 0.0236, V Loss: 0.1076, Accuracy: 0.9679\n",
      "Epoch: 6/20, Step: 7000, T Loss: 0.0545, V Loss: 0.0998, Accuracy: 0.9693\n",
      "Epoch: 7/20, Step: 8000, T Loss: 0.2333, V Loss: 0.0984, Accuracy: 0.9683\n",
      "Epoch: 8/20, Step: 9000, T Loss: 0.0133, V Loss: 0.1022, Accuracy: 0.9655\n",
      "Epoch: 9/20, Step: 10000, T Loss: 0.0366, V Loss: 0.0761, Accuracy: 0.9755\n",
      "Epoch: 10/20, Step: 11000, T Loss: 0.0269, V Loss: 0.0826, Accuracy: 0.9729\n",
      "Epoch: 11/20, Step: 12000, T Loss: 0.0242, V Loss: 0.0733, Accuracy: 0.9781\n",
      "Epoch: 12/20, Step: 13000, T Loss: 0.0023, V Loss: 0.0660, Accuracy: 0.9781\n",
      "Epoch: 12/20, Step: 14000, T Loss: 0.0173, V Loss: 0.0689, Accuracy: 0.9760\n",
      "Epoch: 13/20, Step: 15000, T Loss: 0.0413, V Loss: 0.0718, Accuracy: 0.9769\n",
      "Epoch: 14/20, Step: 16000, T Loss: 0.0015, V Loss: 0.0666, Accuracy: 0.9800\n",
      "Epoch: 15/20, Step: 17000, T Loss: 0.0569, V Loss: 0.0583, Accuracy: 0.9826\n",
      "Epoch: 16/20, Step: 18000, T Loss: 0.0047, V Loss: 0.0673, Accuracy: 0.9769\n",
      "Epoch: 17/20, Step: 19000, T Loss: 0.0034, V Loss: 0.0741, Accuracy: 0.9764\n",
      "Epoch: 17/20, Step: 20000, T Loss: 0.0150, V Loss: 0.0582, Accuracy: 0.9829\n",
      "Epoch: 18/20, Step: 21000, T Loss: 0.0687, V Loss: 0.0657, Accuracy: 0.9807\n",
      "Epoch: 19/20, Step: 22000, T Loss: 0.0359, V Loss: 0.0772, Accuracy: 0.9760\n",
      "Epoch: 20/20, Step: 23000, T Loss: 0.0358, V Loss: 0.0533, Accuracy: 0.9838\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "batch_size = 32\n",
    "epoch = 20\n",
    "print_every = 1000\n",
    "\n",
    "cnn = DigitRecognizerCNN()\n",
    "cnn.make_nn()\n",
    "t_loss, v_loss, acc = cnn.train(epoch, learning_rate, train_x, train_y, batch_size, valid_x, valid_y, get_batch, print_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tensor/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from tensor/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from tensor/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from tensor/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from tensor/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from tensor/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from tensor/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from tensor/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from tensor/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from tensor/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from tensor/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from tensor/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from tensor/model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from tensor/model.ckpt\n",
      "(28000,)\n"
     ]
    }
   ],
   "source": [
    "cnn = DigitRecognizerCNN()\n",
    "cnn.make_nn()\n",
    "size = 2048\n",
    "num_batch = len(test_x) // size + 1\n",
    "predict = []\n",
    "for i in range(num_batch):\n",
    "    p = cnn.predict(test_x[i * size: (i+1) * size])\n",
    "    predict = np.concatenate([predict, p])\n",
    "print(predict.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Make CSV\n",
    "This is just making csv file as kaggle required for submittion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('predict.csv', 'w') as csv_file :\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(['ImageId', 'Label'])\n",
    "    for i in range(len(predict)) :\n",
    "        writer.writerow([str(i+1), str(int(predict[i]))])\n",
    "    csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
