{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron\n",
    "This notebook is quick MLP (multilayer perceptron) predicting digit for [kaggle competition](https://www.kaggle.com/c/digit-recognizer). \n",
    "\n",
    "Please consider follow before reading this notebook.\n",
    "- We don't verify data in this notebook.\n",
    "- MLP isn't the suitable model for image classification, but digit recognition problem is simply enough to use, however for more complex problem consider CNN (Convolution Neural Network)\n",
    "- I would like to make this notebook simple enough for anyone who new to MLP can follow the code easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "train_dataset = pd.read_csv('dataset/train.csv')\n",
    "test_dataset = pd.read_csv('dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset\n",
      "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
      "0      1       0       0       0       0       0       0       0       0   \n",
      "1      0       0       0       0       0       0       0       0       0   \n",
      "2      1       0       0       0       0       0       0       0       0   \n",
      "3      4       0       0       0       0       0       0       0       0   \n",
      "4      0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
      "0       0    ...            0         0         0         0         0   \n",
      "1       0    ...            0         0         0         0         0   \n",
      "2       0    ...            0         0         0         0         0   \n",
      "3       0    ...            0         0         0         0         0   \n",
      "4       0    ...            0         0         0         0         0   \n",
      "\n",
      "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
      "0         0         0         0         0         0  \n",
      "1         0         0         0         0         0  \n",
      "2         0         0         0         0         0  \n",
      "3         0         0         0         0         0  \n",
      "4         0         0         0         0         0  \n",
      "\n",
      "[5 rows x 785 columns]\n",
      "Test Dataset\n",
      "   pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
      "0       0       0       0       0       0       0       0       0       0   \n",
      "1       0       0       0       0       0       0       0       0       0   \n",
      "2       0       0       0       0       0       0       0       0       0   \n",
      "3       0       0       0       0       0       0       0       0       0   \n",
      "4       0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel9    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
      "0       0    ...            0         0         0         0         0   \n",
      "1       0    ...            0         0         0         0         0   \n",
      "2       0    ...            0         0         0         0         0   \n",
      "3       0    ...            0         0         0         0         0   \n",
      "4       0    ...            0         0         0         0         0   \n",
      "\n",
      "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
      "0         0         0         0         0         0  \n",
      "1         0         0         0         0         0  \n",
      "2         0         0         0         0         0  \n",
      "3         0         0         0         0         0  \n",
      "4         0         0         0         0         0  \n",
      "\n",
      "[5 rows x 784 columns]\n",
      "Train Size : 42000\n",
      "Test Size : 28000\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Dataset\")\n",
    "print(train_dataset[:5])\n",
    "print(\"Test Dataset\")\n",
    "print(test_dataset[:5])\n",
    "print(\"Train Size : {}\".format(train_dataset.shape[0]))\n",
    "print(\"Test Size : {}\".format(test_dataset.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data\n",
    "Before training we need to prepare data a little bit. here is the detail\n",
    "- We need to scale each pixel to the range of 0 - 1\n",
    "- Since Kaggle don't have validate data for us,so we have to split some from training data.\n",
    "- We need to spearate label (train_y) out of feature (train_x) so we can feed it to our network as label and feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size : 37800\n",
      "Validation Size : 4200\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "validation_ratio = 0.1\n",
    "\n",
    "pixel_scaler = MinMaxScaler()\n",
    "one_hot = LabelBinarizer()\n",
    "one_hot.fit(range(10))\n",
    "train_y = one_hot.transform(train_dataset['label'])\n",
    "train_x = train_dataset.drop('label', axis=1)\n",
    "train_x = pixel_scaler.fit_transform(train_x)\n",
    "test_x = pixel_scaler.transform(test_dataset)\n",
    "\n",
    "split = int(train_dataset.shape[0] * (1 - validation_ratio))\n",
    "\n",
    "valid_x = train_x[split:train_x.shape[0]]\n",
    "valid_y = train_y[split:train_x.shape[0]]\n",
    "train_x = train_x[0:split]\n",
    "train_y = train_y[0:split]\n",
    "print(\"Train Size : {}\".format(train_x.shape[0]))\n",
    "print(\"Validation Size : {}\".format(valid_x.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Model\n",
    "Below is the model code.\n",
    "- Initialize DigitalRecognizerMLP at first.\n",
    "- Call make_nn() to set up tensorflow graph.\n",
    "- Call train() for train network after training the network has been stored and ready for prediction.\n",
    "- Call predict() for make a prediction with the latest saved network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitRecognizerMLP(object):\n",
    "    def __init__(self, feature_size=784, label_size=10):\n",
    "        self.feature_size = feature_size\n",
    "        self.label_size = label_size\n",
    "    \n",
    "    def build_input(self):\n",
    "        model_x = tf.placeholder(tf.float32, [None, self.feature_size])\n",
    "        model_y = tf.placeholder(tf.int32, [None, self.label_size])\n",
    "        model_lr = tf.placeholder(tf.float32)\n",
    "        return model_x, model_y, model_lr\n",
    "    \n",
    "    def build_mlp(self, model_x, model_y):\n",
    "        hidden_1 = tf.layers.dense(model_x, 1024, activation=tf.nn.relu)\n",
    "        hidden_2 = tf.layers.dense(hidden_1, 512, activation=tf.nn.relu)\n",
    "        hidden_3 = tf.layers.dense(hidden_2, 256, activation=tf.nn.relu)\n",
    "        hidden_4 = tf.layers.dense(hidden_2, 128, activation=tf.nn.relu)\n",
    "        logits = tf.layers.dense(hidden_3, self.label_size, activation=None)\n",
    "        return logits\n",
    "    \n",
    "    def build_output(self, model_y, logits):\n",
    "        output = tf.argmax(logits, axis=1)\n",
    "        loss = tf.losses.softmax_cross_entropy(model_y, logits)\n",
    "        accuracy = tf.reduce_mean( tf.cast( tf.equal( tf.argmax(model_y, axis=1), output ), tf.float32) )\n",
    "        return output, loss, accuracy\n",
    "    \n",
    "    def model_opt(self, model_lr, loss):\n",
    "        opt = tf.train.AdamOptimizer(model_lr).minimize(loss)\n",
    "        return opt\n",
    "    \n",
    "    def make_nn(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.model_x, self.model_y, self.model_lr = self.build_input()\n",
    "        self.logits = self.build_mlp(self.model_x, self.model_y)\n",
    "        self.output, self.loss, self.accuracy = self.build_output(self.model_y, self.logits)\n",
    "        self.opt = self.model_opt(self.model_lr, self.loss)\n",
    "        \n",
    "    def train(self, epoch, learning_rate, train_x, train_y, batch_size, valid_x, valid_y, get_batch_func, print_every):\n",
    "        t_loss_list = []\n",
    "        v_loss_list = []\n",
    "        accuracy_list = []\n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session() as sess :\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            counter = 0\n",
    "            for e in range(epoch) :\n",
    "                for x,y in get_batch_func(train_x, train_y, batch_size):\n",
    "                    feed_dict = {self.model_x:x, self.model_y:y, self.model_lr:learning_rate}\n",
    "                    loss, _ = sess.run([self.loss, self.opt], feed_dict=feed_dict)\n",
    "                    if counter % print_every == 0 :\n",
    "                        feed_dict = {self.model_x:valid_x, self.model_y:valid_y, self.model_lr:learning_rate}\n",
    "                        v_loss, accuracy = sess.run([self.loss, self.accuracy], feed_dict=feed_dict)\n",
    "                        print(\"Epoch: {}/{}, Step: {}, T Loss: {:.4f}, V Loss: {:.4f}, Accuracy: {:.4f}\".format(e+1, \n",
    "                                    epoch, counter, loss, v_loss, accuracy))\n",
    "                        t_loss_list.append( loss )\n",
    "                        v_loss_list.append( v_loss )\n",
    "                        accuracy_list.append( accuracy )\n",
    "                        save_path = saver.save(sess, \"tensor/model.ckpt\")\n",
    "                    counter += 1\n",
    "        return t_loss_list, v_loss_list, accuracy_list\n",
    "    \n",
    "    def predict(self, predict_x):\n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver.restore(sess, \"tensor/model.ckpt\")\n",
    "            feed_dict = {self.model_x:predict_x}\n",
    "            output = sess.run(self.output, feed_dict=feed_dict)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Testing --- Delete after use\n",
    "dr_mlp = DigitRecognizerMLP()\n",
    "dr_mlp.make_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(feature, label, batch_size):\n",
    "    num_batch = feature.shape[0] // batch_size\n",
    "    for i in range(num_batch):\n",
    "        start = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "        x = feature[start:end]\n",
    "        y = label[start:end]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50, Step: 0, T Loss: 2.3151, V Loss: 2.2903, Accuracy: 0.1014\n",
      "Epoch: 2/50, Step: 1000, T Loss: 0.0937, V Loss: 0.1498, Accuracy: 0.9550\n",
      "Epoch: 4/50, Step: 2000, T Loss: 0.0378, V Loss: 0.1087, Accuracy: 0.9671\n",
      "Epoch: 6/50, Step: 3000, T Loss: 0.0262, V Loss: 0.0960, Accuracy: 0.9695\n",
      "Epoch: 7/50, Step: 4000, T Loss: 0.0228, V Loss: 0.0870, Accuracy: 0.9726\n",
      "Epoch: 9/50, Step: 5000, T Loss: 0.1768, V Loss: 0.0906, Accuracy: 0.9729\n",
      "Epoch: 11/50, Step: 6000, T Loss: 0.0057, V Loss: 0.0971, Accuracy: 0.9740\n",
      "Epoch: 12/50, Step: 7000, T Loss: 0.0188, V Loss: 0.1006, Accuracy: 0.9743\n",
      "Epoch: 14/50, Step: 8000, T Loss: 0.0004, V Loss: 0.1107, Accuracy: 0.9729\n",
      "Epoch: 16/50, Step: 9000, T Loss: 0.0022, V Loss: 0.0959, Accuracy: 0.9779\n",
      "Epoch: 17/50, Step: 10000, T Loss: 0.0003, V Loss: 0.1017, Accuracy: 0.9779\n",
      "Epoch: 19/50, Step: 11000, T Loss: 0.0000, V Loss: 0.0972, Accuracy: 0.9790\n",
      "Epoch: 21/50, Step: 12000, T Loss: 0.0001, V Loss: 0.1041, Accuracy: 0.9779\n",
      "Epoch: 23/50, Step: 13000, T Loss: 0.0000, V Loss: 0.1083, Accuracy: 0.9781\n",
      "Epoch: 24/50, Step: 14000, T Loss: 0.0001, V Loss: 0.1099, Accuracy: 0.9795\n",
      "Epoch: 26/50, Step: 15000, T Loss: 0.0000, V Loss: 0.1168, Accuracy: 0.9781\n",
      "Epoch: 28/50, Step: 16000, T Loss: 0.0000, V Loss: 0.1221, Accuracy: 0.9786\n",
      "Epoch: 29/50, Step: 17000, T Loss: 0.0000, V Loss: 0.1282, Accuracy: 0.9776\n",
      "Epoch: 31/50, Step: 18000, T Loss: 0.0000, V Loss: 0.1261, Accuracy: 0.9790\n",
      "Epoch: 33/50, Step: 19000, T Loss: 0.0000, V Loss: 0.1418, Accuracy: 0.9776\n",
      "Epoch: 34/50, Step: 20000, T Loss: 0.0000, V Loss: 0.1405, Accuracy: 0.9795\n",
      "Epoch: 36/50, Step: 21000, T Loss: 0.0000, V Loss: 0.1426, Accuracy: 0.9800\n",
      "Epoch: 38/50, Step: 22000, T Loss: 0.0000, V Loss: 0.1568, Accuracy: 0.9800\n",
      "Epoch: 39/50, Step: 23000, T Loss: 0.0000, V Loss: 0.1590, Accuracy: 0.9798\n",
      "Epoch: 41/50, Step: 24000, T Loss: 0.0000, V Loss: 0.1627, Accuracy: 0.9790\n",
      "Epoch: 43/50, Step: 25000, T Loss: 0.0000, V Loss: 0.1704, Accuracy: 0.9795\n",
      "Epoch: 45/50, Step: 26000, T Loss: 0.0000, V Loss: 0.1753, Accuracy: 0.9788\n",
      "Epoch: 46/50, Step: 27000, T Loss: 0.0000, V Loss: 0.1795, Accuracy: 0.9786\n",
      "Epoch: 48/50, Step: 28000, T Loss: 0.0000, V Loss: 0.1828, Accuracy: 0.9788\n",
      "Epoch: 50/50, Step: 29000, T Loss: 0.0000, V Loss: 0.1913, Accuracy: 0.9781\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "batch_size = 64\n",
    "epoch = 50\n",
    "print_every = 1000\n",
    "\n",
    "mlp = DigitRecognizerMLP()\n",
    "mlp.make_nn()\n",
    "t_loss, v_loss, acc = mlp.train(epoch, learning_rate, train_x, train_y, batch_size, valid_x, valid_y, get_batch, print_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tensor/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "mlp = DigitRecognizerMLP()\n",
    "mlp.make_nn()\n",
    "predict = mlp.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Make CSV\n",
    "This is just making csv file as kaggle required for submittion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('predict.csv', 'w') as csv_file :\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(['ImageId', 'Label'])\n",
    "    for i in range(len(predict)) :\n",
    "        writer.writerow([str(i+1), str(predict[i])])\n",
    "    csv_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
